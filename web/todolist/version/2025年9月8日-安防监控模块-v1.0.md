### **产品需求文档 (PRD): 无人咖啡机器人安防监控系统 - 算法模型 V1.0**

**1. 项目概述**

*   **1.1 项目目标:**
    为无人咖啡机器人开发或集成一个部署在边缘端（工控机）的轻量级深度学习模型。该模型需能实时分析摄像头捕捉的视频流，准确检测并分类可能对设备造成损害的人员异常行为，为后续的预警干预（如语音播报）提供决策依据。

*   **1.2 核心挑战:**
    *   **资源限制:** 模型必须在算力有限的工控机上流畅运行，不能显著影响机器人本身的业务逻辑。
    *   **实时性要求:** 从画面输入到结果输出的延迟必须足够低，以保证预警的及时性。
    *   **场景复杂性:** 外部环境光照变化大、可能存在多人、有物体遮挡等情况，模型需具备良好的鲁棒性。
    *   **行为多样性:** “打砸”等恶意行为定义宽泛，动作形态各异，模型需具备较好的泛化能力。

**2. 算法功能需求**

*   **2.1 基础能力 - 目标检测**
    *   **2.1.1 人体检测 (Human Detection):**
        *   **需求描述:** 模型必须能够准确地从视频帧中检测出单人或多人的位置（边界框）。这是所有后续行为分析的基础。
        *   **验收标准:** 在机器人前方有效监控范围内，对清晰可见的人体目标检测准确率（Precision）不低于95%，召回率（Recall）不低于90%。

*   **2.2 核心能力 - 异常行为识别**
    *   **2.2.1 行为分类定义:**
        *   **需求描述:** 模型需要对检测到的人体目标进行行为姿态分析，并至少识别以下三类预定义的异常行为。
        *   **P0级 - 剧烈攻击行为:**
            *   **具体动作:** 快速挥拳或使用物体砸向设备、抬脚踹踢设备、猛烈摇晃或撞击设备。
            *   **特点:** 动作幅度大、速度快、具有明显的攻击意图。
        *   **P1级 - 潜在攻击或威胁行为:**
            *   **具体动作:** 高举手臂（超过头部）并长时间保持（如持物欲砸）、用力拍打设备表面、在设备前做大幅度的挥舞动作。
            *   **特点:** 具有攻击前兆，或对设备构成潜在威胁。
        *   **P2级 - 可疑徘徊行为:**
            *   **具体动作:** 用户在非设备交互区域（如设备侧后方）长时间逗留（例如超过1分钟）、反复进行窥探性动作。
            *   **特点:** 行为模式与正常顾客不符，存在潜在风险。
    *   **2.2.2 正常行为过滤:**
        *   **需求描述:** 模型必须能够区分正常的用户行为，避免误报。
        *   **需要过滤的正常行为（示例）:** 正常的用户交互（触摸屏幕、取咖啡）、用户整理衣物、背包、打电话、儿童在旁边的正常活动等。
    *   **交付物:** 详细的、可量化的各类异常/正常行为定义说明文档，包含图片或短视频示例。

**3. 算法性能指标**

*   **3.1 准确性指标:**
    *   **P0级行为识别准确率:** > 95% （此类行为必须精准识别，宁可错报，不可漏报）。
    *   **P1级行为识别准确率:** > 90%。
    *   **整体误报率 (False Positive Rate):** 在连续24小时的真实场景（或高度模拟场景）测试中，由正常行为引起的误报警次数应少于5次。

*   **3.2 实时性指标:**
    *   **模型推理时延 (Inference Time):** 在目标工控机上，单帧画面的处理时间（从输入到输出）应低于100毫秒（ms），以确保系统能达到至少10 FPS的处理速度。

*   **3.3 轻量化与资源占用指标:**
    *   **模型大小:** 模型的权重文件大小应尽可能小，便于部署和更新。
    *   **CPU占用率:** 在模型运行时，其峰值CPU占用率不应超过工控机总CPU资源的50%。
    *   **内存占用:** 运行时内存占用应控制在合理范围内（例如，不超过512MB，需根据工控机实际配置确定）。

**4. 技术与开发需求**

*   **4.1 模型架构选型:**
    *   **建议方案:**
        *   **两阶段方案:** 先使用一个轻量级的通用目标检测模型（如 YOLOv5/v7/v8 的 nano/small 版本, MobileNet-SSD 等）进行人体检测，然后对检测到的人体区域进行姿态估计（如 OpenPifPaf, MoveNet 等）或使用裁剪的图像进行分类。
        *   **一阶段方案:** 直接使用基于时序分析的模型（如 SlowFast, C3D 等的轻量化版本）进行端到端的行为识别。
    *   **要求:** 算法工程师需进行技术评估，选择最适合当前硬件和场景的方案，并给出充分理由。

*   **4.2 训练数据:**
    *   **数据来源:**
        *   **公开数据集:** 利用现有的行为识别公开数据集（如 UCF101, HMDB51, Kinetics 等）进行预训练。
        *   **自建/采集数据集:** **（核心工作）** 必须在真实的机器人部署场景下采集数据，或高度模拟场景进行数据录制。数据需要覆盖不同光照、不同角度、不同着装、多人场景。
        *   **数据增强:** 通过翻转、裁剪、亮度调整、模拟遮挡等技术手段扩充数据集。
    *   **数据标注:**
        *   所有采集的数据都需要进行精细的标注，包括人体边界框和对应的行为类别（P0/P1/P2/正常）。
        *   需要制定详细的数据标注规范文档。
    *   **交付物:** 数据集构建方案、数据标注规范文档、最终用于训练的数据集。

*   **4.3 开发与部署:**
    *   **开发框架:** 建议使用主流的深度学习框架，如 PyTorch 或 TensorFlow。
    *   **部署工具:** 需要考虑使用 ONNX、TensorRT、OpenVINO 等工具对训练好的模型进行优化和加速，以满足在工控机上的性能要求。
    *   **模型接口:**
        *   **输入:** 接受单帧图像数据（如 OpenCV Mat 格式）。
        *   **输出:** 返回一个结构化的数据格式（如 JSON），包含检测到的每个人员信息，如下所示：
            ```json
            [
              {
                "id": 1,
                "box": [x1, y1, x2, y2], // 边界框坐标
                "confidence": 0.98, // 人体检测置信度
                "action": "P0_ATTACK", // 行为分类结果
                "action_confidence": 0.95 // 行为分类置信度
              }
            ]
            ```

**5. 交付物与验收**

*   **5.1 文档交付:**
    *   算法技术选型与设计方案文档。
    *   数据集构建与标注规范文档。
    *   模型训练报告（包含训练过程、参数、最终性能指标）。
    *   模型部署与接口调用说明文档。

*   **5.2 代码与模型交付:**
    *   完整的、带有注释的训练代码和推理代码。
    *   最终优化好的、可直接部署的模型文件。

*   **5.3 验收标准:**
    *   在指定的工控机硬件上成功部署模型服务。
    *   通过编写的测试用例，模型的各项性能指标（准确率、实时性、资源占用）均达到或优于本PRD中定义的要求。
    *   在模拟场景和真实场景下进行综合测试，系统表现稳定、可靠。